{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "The purpose of this notebook is to apply post-hoc data fixes to results data. Any modification to the data must be retained in this notebook. No modification to performance metrics will be made. Only modifications to independent variables, or additions of new performance metrics will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NlSqlBenchmark.NlSqlBenchmarkFactory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kluoma/skalpel-subsetting-evaluation/venv/lib/python3.10/site-packages/snowflake/connector/options.py:108: UserWarning: You have an incompatible version of 'pyarrow' installed (19.0.1), please install a version that adheres to: 'pyarrow<19.0.0; extra == \"pandas\"'\n",
      "  warn_incompatible_dep(\n",
      "/data/kluoma/skalpel-subsetting-evaluation/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from NlSqlBenchmark.snails.util import sqlite_db_util\n",
    "from util.load_subsets_from_results import load_subsets_from_results\n",
    "from NlSqlEvaluator.NlSqlPromptBuilder import NlSqlPromptBuilder\n",
    "from NlSqlBenchmark.SchemaObjects import Schema, SchemaTable, TableColumn, ForeignKey\n",
    "from NlSqlBenchmark.BenchmarkQuestion import BenchmarkQuestion\n",
    "from NlSqlBenchmark.NlSqlBenchmarkFactory import NlSqlBenchmarkFactory\n",
    "from SchemaSubsetter.Skalpel.LLM import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this once!\n",
    "cwd = os.getcwd()\n",
    "cwd = os.path.dirname(cwd)\n",
    "os.chdir(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add prompt token counts to already-completed CHESS subsetting runs\n",
    "\n",
    "This traverses the CHESS logs and counts the tokens in the prompts used to generate subsets. It saves the counts to the results token_count column for each generated subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SNAILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 503/503 [1:11:45<00:00,  8.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from SchemaSubsetter.ChessSubsetter import ChessSubsetter\n",
    "results_file = \"./subsetting_results/archive/subsetting-chess-snails-Native-gpt4o.xlsx\"\n",
    "results_df = pd.read_excel(results_file)\n",
    "for row in tqdm(results_df.itertuples(), total=results_df.shape[0]):\n",
    "    if row.prompt_tokens == 0:\n",
    "        token_counts, total_tokens = ChessSubsetter.get_token_counts_from_log(row.database, row.question_number)\n",
    "    else:\n",
    "        total_tokens = row.prompt_tokens\n",
    "    results_df.at[row.Index, \"prompt_tokens\"] = total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel(\"./subsetting_results/subsetting-chess-snails-Native-gpt4o.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BIRD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NlSqlBenchmark.NlSqlBenchmarkFactory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kyle.luoma\\Research Projects\\SKALPEL-subsetting-evaluation\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 1534/1534 [35:21<00:00,  1.38s/it]\n"
     ]
    }
   ],
   "source": [
    "from SchemaSubsetter.ChessSubsetter import ChessSubsetter\n",
    "results_file = \"./subsetting_results/archive/subsetting-chess-bird-Native-gpt4o.xlsx\"\n",
    "results_df = pd.read_excel(results_file)\n",
    "for row in tqdm(results_df.itertuples(), total=results_df.shape[0]):\n",
    "    if row.prompt_tokens == 0:\n",
    "        token_counts, total_tokens = ChessSubsetter.get_token_counts_from_log(row.database, row.question_number)\n",
    "    else:\n",
    "        total_tokens = row.prompt_tokens\n",
    "    results_df.at[row.Index, \"prompt_tokens\"] = total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel(\"./subsetting_results/subsetting-chess-bird-Native-gpt4o.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index the Snails NYSED database to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sqlite_db_util.index_nysed_db(db_list_file=\"./benchmarks/snails/snails_sqlite/sqlite_dbinfo.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derive prompt token counts for subset schema prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kluoma/skalpel-subsetting-evaluation/src/util/StringObjectParser.py:58: UserWarning: Using eval can be dangerous. Make sure the input string is from a trusted source.\n",
      "  warnings.warn(\"Using eval can be dangerous. Make sure the input string is from a trusted source.\")\n"
     ]
    }
   ],
   "source": [
    "nl_sql_files = [f for f in os.listdir(\"./nl_sql_results\") if \".xlsx\" in f]\n",
    "subsetting_files = [f.replace(\"nltosql\", \"subsetting\").split(\"-nlsqlmodel\")[0] + \".xlsx\" for f in nl_sql_files]\n",
    "bm_fact = NlSqlBenchmarkFactory()\n",
    "prompt_builder = NlSqlPromptBuilder()\n",
    "llm = LLM()\n",
    "for nlsql_f, subset_f in list(zip(nl_sql_files, subsetting_files)):\n",
    "    nl_sql_prompt_tokens = []\n",
    "    if \"fullschema\" in nlsql_f:\n",
    "        continue\n",
    "    bm_name = nlsql_f.split(\"-\")[2]\n",
    "    bm = bm_fact.build_benchmark(benchmark_name=bm_name)\n",
    "    subsets = load_subsets_from_results(\"./subsetting_results/\" + subset_f, bm)\n",
    "    for subset, question in subsets[1]:\n",
    "        quest_sub = question\n",
    "        quest_sub.schema = subset\n",
    "        prompt = prompt_builder.create_prompt(quest_sub)\n",
    "        token_count = llm.get_prompt_token_count(prompt)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
